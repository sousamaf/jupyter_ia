{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "apparent-segment",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "tabela_q = np.zeros([10, 6])\n",
    "estado = 1\n",
    "acao = 1\n",
    "proximo_estado = 1\n",
    "alpha = 0.2\n",
    "gamma = 0.8\n",
    "premiacao = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indie-future",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Aprendizado por Reforço\n",
    "\n",
    "Contexto e aplicação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "motivated-interval",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Agente e o ambiente](http://www.jbocinsky.com/assets/images/DoubleQLearningFlowDiagram.png)\n",
    "\n",
    "##  O ambiente do simulador Gym\n",
    "- o agente executará ações no ambiente;\n",
    "- o ambiente retornará o estado do agente e recompensas.\n",
    "\n",
    "## Aprendizado por exploração\n",
    "- construção de experiência durante a exploração e interação com o ambiente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leading-portland",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Aplicações que fazem uso de aprendizado por reforço\n",
    "- Reconhecimento de fala\n",
    "- Reconhecimento facial\n",
    "- Recomendações personalizadas\n",
    "- Diagnósticos no setor de saúde\n",
    "- Recomendação de notícias\n",
    "\n",
    "Notícia: https://mundomaistech.com.br/inteligencia-artificial/5-aplicacoes-cotidianas-que-utilizam-deep-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eleven-vaccine",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Q-Learning\n",
    "\n",
    "Um algoritmo de aprendizado por reforço, o mais popular. \n",
    "\n",
    "Em uma curta explicação: o algoritmo busca encontrar a melhor ação a ser executada a partir do estado atual."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dietary-theory",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Como ocorre o aprendizado?\n",
    "\n",
    "Através da Tabela Q o agente obtem a melhor ação para o momento.\n",
    "\n",
    "# Por que 'Q'\n",
    "O 'Q' significa qualidade, no contexto de uma ação, o quão útil uma determinada ação é para a obtenção de uma recompensa.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automated-powder",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Q-Learning](https://perfectial.com/wp-content/uploads/2019/10/q_learning-01.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innovative-optimum",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Como ocorre a manutenção dos valores da tabela Q?\n",
    "\n",
    "- Inicialmente a tabela está vazia.\n",
    "- A cada nova interação com o ambiente o agente atualiza seus valores.\n",
    "\n",
    "# Exploration or Exploitation\n",
    "- Ao utilizar os valores da tabela Q o agente toma uma ação de exploitation\n",
    "- Ao tomar uma ação aleatória, o agente realiza uma exploration.\n",
    "\n",
    "### Para viabilizar a variação entre exploitation e exploration é definido o parâmetro:\n",
    "$$epsilon$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "opponent-provincial",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "epsilon = 0.2\n",
    "if random.uniform(0, 1) < epsilon:\n",
    "    \"\"\"\n",
    "    Explore: select a random action\n",
    "    \"\"\"\n",
    "else:\n",
    "    \"\"\"\n",
    "    Exploit: select the action with max value (future reward)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confirmed-arabic",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Atualização da tabela Q\n",
    "\n",
    "$$novoValor = (1 - alpha) * valor + alpha * (premiacao + gamma * maiorValor)$$\n",
    "\n",
    "onde:\n",
    "- alpha: taxa de ganho defini o quanto você aceita o novo valor versos o antigo.\n",
    "- gamma: um fator de desconto para equilibrar recompensas futuras.\n",
    "- premiacao: valor recebido após cumprir a ação.\n",
    "- maiorValor: recompensa futura máxima.\n",
    "- valor: recompensa atual da tabela Q."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "rising-looking",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "valor = tabela_q[estado, acao]\n",
    "maior_valor = np.max(tabela_q[proximo_estado])\n",
    "\n",
    "novo_valor = (1 - alpha) * valor + alpha * (premiacao + gamma * maior_valor)\n",
    "tabela_q[estado, acao] = novo_valor\n",
    "estado = proximo_estado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attended-wallet",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Próximo passo?\n",
    "\n",
    "Construção de métricas para avaliar o desempenho da aprendizagem.\n",
    "\n",
    "# Como representar o conhecimento em outros contextos?\n",
    "Como definir o universo de ações possíveis em outros contextos possíveis para a aprendizagem por reforço"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electronic-cable",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
